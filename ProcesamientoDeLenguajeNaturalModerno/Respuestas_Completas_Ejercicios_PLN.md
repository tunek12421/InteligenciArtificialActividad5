# Respuestas Completas - Ejercicios de Procesamiento de Lenguaje Natural

## Ejercicio 1: Word2Vec

**Resultados de Ejecuci√≥n:**
- Vocabulario: 11,598 palabras
- Palabras similares a 'good': funny (0.905), bad (0.895), great (0.861)
- Tiempo optimizado con 8 workers (paralelizaci√≥n completa)

### Preguntas y Respuestas:

**1. ¬øQu√© representa un vector de palabras en Word2Vec?**
Un vector num√©rico multidimensional que captura el significado sem√°ntico de la palabra basado en los contextos donde aparece. Cada dimensi√≥n representa una caracter√≠stica sem√°ntica latente.

**2. ¬øCu√°l es la diferencia entre el enfoque CBOW y Skip-Gram?**
- **CBOW (Continuous Bag of Words):** Predice la palabra central bas√°ndose en las palabras del contexto circundante. M√°s r√°pido para corpus grandes.
- **Skip-Gram:** Predice las palabras del contexto bas√°ndose en la palabra central. Mejor para corpus peque√±os y palabras raras.

**3. ¬øQu√© significa que dos palabras tengan vectores "cercanos"?**
Significa que tienen significados similares o aparecen en contextos parecidos. La cercan√≠a se mide con similitud coseno (valores cercanos a 1.0 indican alta similitud sem√°ntica).

**4. ¬øC√≥mo influye el par√°metro window en el entrenamiento?**
Define el tama√±o de la ventana de contexto (cu√°ntas palabras a cada lado se consideran). Una ventana de 5 significa que se consideran 10 palabras totales alrededor de la palabra objetivo. Ventanas m√°s grandes capturan relaciones sem√°nticas m√°s amplias.

**5. ¬øPor qu√© es necesario hacer tokenizaci√≥n antes de entrenar?**
Convierte el texto continuo en unidades discretas (tokens/palabras) que el modelo puede procesar matem√°ticamente. Sin tokenizaci√≥n, el modelo no puede identificar l√≠mites de palabras ni crear el vocabulario.

---

## Ejercicio 2: GloVe (Embeddings Preentrenados)

**Resultados de Ejecuci√≥n:**
- Vocabulario: 100,000 palabras cargadas en 6.77 segundos
- Similitudes: king-queen (0.75), cat-banana (0.27), car-vehicle (0.86)
- Analog√≠a exitosa: king - man + woman = queen (0.77)

### Preguntas y Respuestas:

**1. ¬øCu√°l es la diferencia entre GloVe y Word2Vec en cuanto a su forma de entrenamiento?**
- **GloVe:** Utiliza estad√≠sticas globales de co-ocurrencia de todo el corpus. Factoriza una matriz de co-ocurrencia palabra-palabra.
- **Word2Vec:** Utiliza ventanas locales de contexto. Entrena con pares palabra-contexto secuenciales.

**2. ¬øPor qu√© usamos KeyedVectors en este ejercicio?**
KeyedVectors es una interfaz optimizada para cargar y usar embeddings preentrenados sin necesidad de reentrenar el modelo. Permite b√∫squedas eficientes de similitud y analog√≠as.

**3. ¬øQu√© resultados obtuviste al comparar "king" y "queen"? ¬øQu√© interpretas?**
Similitud de 0.75 (alta), indicando que el modelo captura correctamente la relaci√≥n sem√°ntica entre conceptos de realeza. Los embeddings entienden roles sociales similares.

**4. ¬øPuedes mencionar un caso donde el an√°lisis sem√°ntico con GloVe ser√≠a √∫til en la industria?**
- **E-commerce:** B√∫squeda sem√°ntica de productos ("zapatillas deportivas" encuentra "tennis", "running shoes")
- **Marketing:** An√°lisis de sentimientos en redes sociales y agrupaci√≥n de menciones similares
- **Recursos Humanos:** Matching autom√°tico de CVs con ofertas de trabajo

**5. ¬øQu√© limitaciones tienen los embeddings est√°ticos como GloVe?**
- Una sola representaci√≥n por palabra (no maneja polisemia: "banco" financiero vs "banco" asiento)
- No se actualizan con nuevos contextos
- Sesgo inherente del corpus de entrenamiento
- No capturan contexto din√°mico de la oraci√≥n

---

## Ejercicio 3: Embeddings Personalizados

**Resultados de Ejecuci√≥n:**
- Dataset: 279,577 art√≠culos de Medium sobre IA/ML/DS
- Vocabulario personalizado: 4,220 palabras t√©cnicas
- Relaciones fuertes: intelligence-artificial (0.89), data-science (0.67)

### Preguntas y Respuestas:

**1. ¬øPor qu√© podr√≠as preferir entrenar tus propios embeddings en vez de usar GloVe?**
- **Vocabulario espec√≠fico:** Captura jerga y terminolog√≠a particular de tu dominio
- **Contexto relevante:** Relaciones sem√°nticas espec√≠ficas de tu industria/aplicaci√≥n
- **Datos actualizados:** Refleja tendencias y t√©rminos actuales, no datos hist√≥ricos
- **Control total:** Puedes ajustar par√°metros seg√∫n tus necesidades espec√≠ficas

**2. ¬øQu√© caracter√≠sticas del texto pueden afectar la calidad de los embeddings?**
- **Limpieza:** Textos con ruido (HTML, caracteres especiales) degradan calidad
- **Tama√±o del corpus:** Muy peque√±o produce embeddings poco robustos
- **Diversidad:** Falta de variedad contextual limita las representaciones
- **Frecuencia:** Palabras muy raras o muy comunes pueden ser problem√°ticas

**3. ¬øC√≥mo se refleja el dominio del texto en los vectores obtenidos?**
Los t√©rminos t√©cnicos del dominio (IA/ML) muestran alta similitud entre s√≠. "intelligence" y "artificial" tienen 0.89 de similitud, reflejando su co-ocurrencia frecuente en el corpus especializado.

**4. ¬øQu√© cambios har√≠as para mejorar la calidad de tus embeddings?**
- **M√°s datos:** Incluir m√°s art√≠culos del dominio
- **Mejor limpieza:** Preprocesamiento m√°s sofisticado (lemmatizaci√≥n, eliminaci√≥n de stopwords)
- **Ajustar par√°metros:** Experimentar con dimensiones del vector, tama√±o de ventana
- **Filtrado:** Eliminar palabras muy raras o muy comunes

**5. ¬øQu√© usos pr√°cticos tendr√≠a este modelo dentro de una empresa?**
- **B√∫squeda documental:** Encontrar documentos t√©cnicos similares
- **Clustering:** Agrupar tickets de soporte por tema
- **Recomendaciones:** Sugerir art√≠culos relacionados a empleados
- **An√°lisis de tendencias:** Identificar temas emergentes en feedback

---

## Ejercicio 5: Resumen Autom√°tico con BART

**Resultados de Ejecuci√≥n:**
- Modelo: facebook/bart-large-cnn ejecutando en GPU
- Memoria GPU utilizada: 0.76 GB
- Res√∫menes generated exitosamente para m√∫ltiples textos

### Preguntas y Respuestas:

**1. ¬øC√≥mo se diferencia el resumen extractivo del resumen abstractivo?**
- **Extractivo:** Selecciona y copia frases exactas del texto original. M√°s conservador pero menos natural.
- **Abstractivo:** Genera texto nuevo parafraseando y condensando ideas. M√°s natural pero puede introducir imprecisiones.

**2. ¬øPor qu√© usamos facebook/bart-large-cnn para esta tarea?**
BART-large-cnn fue espec√≠ficamente fine-tuned en el dataset CNN/DailyMail para tareas de resumen. Combina las fortalezas de BERT (comprensi√≥n) y GPT (generaci√≥n), optimizado para res√∫menes de noticias.

**3. ¬øQu√© limitaciones encontraste en los res√∫menes generados?**
- Puede perder detalles importantes para mantener brevedad
- Ocasionalmente genera informaci√≥n no presente en el original
- Limitado por el contexto m√°ximo del modelo (1024 tokens)
- Sesgado hacia el estilo de noticias (CNN/DailyMail)

**4. ¬øC√≥mo podr√≠as ajustar el modelo para res√∫menes m√°s cortos o m√°s largos?**
- **Par√°metros:** Ajustar `max_length` y `min_length`
- **Sampling:** Usar `do_sample=True` con `temperature` para variabilidad
- **Beam search:** Ajustar `num_beams` para mejor calidad
- **Length penalty:** Controlar preferencia por res√∫menes largos/cortos

**5. ¬øEn qu√© aplicaciones reales ser√≠a √∫til esta t√©cnica?**
- **Medios:** Generaci√≥n autom√°tica de abstracts de noticias
- **Legal:** Res√∫menes de documentos legales extensos
- **Investigaci√≥n:** Abstracts autom√°ticos de papers acad√©micos
- **Empresas:** Res√∫menes ejecutivos de reportes largos

---

## Ejercicio 6: An√°lisis de Sentimientos con DistilBERT

**Resultados de Ejecuci√≥n:**
- Modelo ejecutando en GPU correctamente
- Precisi√≥n: >99% de confianza en frases claras
- Procesamiento en lote de 8 frases simult√°neamente

### Preguntas y Respuestas:

**1. ¬øQu√© ventajas tiene DistilBERT sobre BERT completo?**
- **Tama√±o:** 60% m√°s peque√±o (66M vs 110M par√°metros)
- **Velocidad:** 60% m√°s r√°pido en inferencia
- **Rendimiento:** Mantiene 97% del rendimiento de BERT
- **Memoria:** Menor consumo de GPU/CPU

**2. ¬øQu√© tipo de tareas reales puedes resolver con an√°lisis de sentimientos?**
- **E-commerce:** An√°lisis autom√°tico de reviews de productos
- **Redes sociales:** Monitoreo de marca y reputaci√≥n online
- **Atenci√≥n al cliente:** Priorizaci√≥n autom√°tica de tickets negativos
- **Investigaci√≥n de mercado:** An√°lisis de opiniones sobre productos/servicios

**3. ¬øQu√© nivel de confianza obtuviste para las frases positivas/negativas?**
Confianza muy alta (>99%) para frases con sentimiento claro. Frases neutras como "The movie was okay" mostraron menor confianza pero correcta clasificaci√≥n.

**4. ¬øEn qu√© casos podr√≠a fallar un modelo de sentimiento?**
- **Sarcasmo e iron√≠a:** "¬°Qu√© excelente servicio!" (sarc√°stico)
- **Contexto cultural:** Referencias locales o culturales espec√≠ficas
- **Emociones mixtas:** Opiniones que combinan aspectos positivos y negativos
- **Negaci√≥n compleja:** Dobles negaciones o negaciones sutiles

**5. ¬øQu√© cambios podr√≠as hacer para adaptarlo a un nuevo idioma?**
- **Modelos multiling√ºes:** Usar mBERT o XLM-RoBERTa
- **Fine-tuning:** Entrenar con datos espec√≠ficos del idioma
- **Traducci√≥n:** Traducir texto al ingl√©s, analizar, y mapear resultado
- **Modelos nativos:** Usar modelos preentrenados espec√≠ficos del idioma

---

## Ejercicio 7: Question Answering con BERT

**Resultados de Ejecuci√≥n:**
- Modelo DistilBERT-SQuAD ejecutando en GPU
- Memoria GPU: 0.13 GB utilizada
- Respuestas encontradas con confianza variable (0.10-0.63)

### Preguntas y Respuestas:

**1. ¬øQu√© hace el modelo para identificar la respuesta dentro del contexto?**
El modelo identifica spans (segmentos) de texto que probablemente contienen la respuesta. Predice tokens de inicio y fin, calculando probabilidades para cada posici√≥n posible en el contexto.

**2. ¬øPor qu√© es √∫til tener un modelo preentrenado en SQuAD?**
SQuAD contiene 100,000+ preguntas-respuestas de alta calidad anotadas por humanos. El preentrenamiento proporciona:
- Comprensi√≥n de patrones pregunta-respuesta
- Transfer learning para nuevos dominios
- Base s√≥lida para fine-tuning espec√≠fico

**3. ¬øQu√© tan preciso fue el modelo en tus pruebas?**
Precisi√≥n variable seg√∫n complejidad:
- Respuestas directas (fechas, nombres): Alta precisi√≥n
- Conceptos abstractos: Precisi√≥n moderada
- Preguntas ambiguas: Menor confianza pero respuestas relevantes

**4. ¬øQu√© desaf√≠os enfrentar√≠as si quisieras entrenar tu propio modelo de QA?**
- **Datos anotados:** Necesitas miles de pares pregunta-respuesta anotados manualmente
- **Costo computacional:** Entrenamiento requiere GPU potentes durante d√≠as/semanas
- **Expertise:** Conocimiento profundo de arquitecturas transformer
- **Evaluaci√≥n:** M√©tricas complejas (F1, EM) para validar calidad

**5. ¬øPuedes imaginar una aplicaci√≥n de esta t√©cnica en tu entorno profesional?**
- **Soporte t√©cnico:** FAQ autom√°tico que responde preguntas de usuarios
- **Documentaci√≥n:** Sistema de b√∫squeda inteligente en manuales t√©cnicos
- **Legal:** B√∫squeda de precedentes y respuestas en documentos legales
- **Educaci√≥n:** Asistente virtual para estudiantes con dudas acad√©micas

---

## Ejercicio 8: Chatbot con DialoGPT

**Resultados de Ejecuci√≥n:**
- GPU configurado y listo para inferencia
- Modelo DialoGPT-medium optimizado para conversaciones
- Demo funcional con ejemplos de conversaci√≥n

### Preguntas y Respuestas:

**1. ¬øQu√© diferencia a un chatbot basado en reglas de uno basado en modelos generativos como DialoGPT?**
- **Basado en reglas:** Respuestas predefinidas, limitado a patrones espec√≠ficos, predecible pero r√≠gido
- **Generativo (DialoGPT):** Crea respuestas nuevas, m√°s natural y flexible, pero puede ser impredecible o generar contenido inapropiado

**2. ¬øC√≥mo maneja el modelo el historial de la conversaci√≥n?**
Concatena la conversaci√≥n previa como contexto para la siguiente respuesta. Mantiene memoria de intercambios anteriores hasta el l√≠mite de tokens del modelo (t√≠picamente 3-5 intercambios).

**3. ¬øQu√© problemas encontraste en la coherencia de las respuestas?**
- P√©rdida de contexto despu√©s de varias interacciones
- Respuestas repetitivas o gen√©ricas
- Dificultad para mantener personalidad consistente
- Puede generar informaci√≥n contradictoria

**4. ¬øQu√© har√≠as para mejorar la fluidez y precisi√≥n del chatbot?**
- **Fine-tuning:** Entrenar con datos espec√≠ficos del dominio
- **Filtros de seguridad:** Evitar respuestas inapropiadas
- **Memoria extendida:** Sistemas de memoria a largo plazo
- **Retrieval-augmented:** Combinar con base de conocimientos externa

**5. ¬øQu√© otros modelos podr√≠as probar en lugar de DialoGPT?**
- **GPT-3.5/4:** M√°s capaces pero requieren API
- **LLaMA:** Modelo open-source m√°s reciente
- **Claude:** Excelente para conversaciones naturales
- **Blenderbot:** Especializado en conversaciones casuales

---

## Ejercicio 9: Pipeline Integrador (Clasificaci√≥n + Resumen + Sentimientos)

**Resultados de Ejecuci√≥n:**
- Pipeline completo integrando 3 tareas de NLP
- GPU configurado para todos los modelos
- Procesamiento de m√∫ltiples textos con m√©tricas completas

### Preguntas y Respuestas:

**1. ¬øQu√© tarea result√≥ m√°s precisa: el resumen, la clasificaci√≥n o el an√°lisis de sentimientos?**
**Clasificaci√≥n tem√°tica** fue la m√°s precisa (0.88-0.92 confianza), seguida por an√°lisis de sentimientos (0.67-0.91). El resumen, siendo generativo, es m√°s dif√≠cil de evaluar objetivamente.

**2. ¬øQu√© tan bien se adaptaron los modelos preentrenados a tu texto personalizado?**
Muy bien para contenido general. Los modelos preentrenados en datasets diversos manejan correctamente textos sobre tecnolog√≠a, econom√≠a y salud. Menor precisi√≥n esperada para jerga muy espec√≠fica.

**3. ¬øC√≥mo integrar√≠as este pipeline en una aplicaci√≥n web real?**
- **API REST:** Endpoints separados para cada tarea (/summarize, /sentiment, /classify)
- **Microservicios:** Cada modelo en contenedor independiente
- **Cache:** Redis para cachear resultados de textos frecuentes
- **Load balancing:** Distribuir carga entre m√∫ltiples instancias GPU

**4. ¬øQu√© parte del pipeline automatizar√≠as o optimizar√≠as con otra herramienta?**
- **Preprocesamiento:** Apache Kafka para streaming de datos
- **Batch processing:** Apache Spark para procesar grandes vol√∫menes
- **Monitoring:** MLflow para tracking de m√©tricas y versiones
- **Orquestaci√≥n:** Kubernetes para scaling autom√°tico

**5. ¬øQu√© mejoras podr√≠as hacer si el texto estuviera en otro idioma o jerga regional?**
- **Modelos multiling√ºes:** mBERT, XLM-R, mT5 para m√∫ltiples idiomas
- **Detecci√≥n de idioma:** Identificar autom√°ticamente el idioma del texto
- **Fine-tuning local:** Entrenar con datos espec√≠ficos de la regi√≥n/jerga
- **Traducci√≥n:** Pipeline de traducci√≥n autom√°tica + procesamiento en ingl√©s

---

## Sugerencias de Mejora Implementadas

### Para 3 Ejercicios Espec√≠ficos:

**Ejercicio 1 (Word2Vec):**
- ‚úÖ Visualizaci√≥n con m√©tricas de similitud detalladas
- ‚úÖ Confirmaci√≥n de uso de Skip-Gram por defecto
- ‚úÖ Paralelizaci√≥n optimizada con todos los cores disponibles

**Ejercicio 2 (GloVe):**
- ‚úÖ Ejemplo de analog√≠a implementado (king - man + woman ‚âà queen)
- ‚úÖ Comentarios sobre dimensionalidad de vectores (100d)
- ‚úÖ M√∫ltiples pares de similitud para an√°lisis comparativo

**Ejercicio 3 (Embeddings Personalizados):**
- ‚úÖ An√°lisis de frecuencia y limpieza de texto implementado
- ‚úÖ Comentarios detallados sobre preprocesamiento
- ‚úÖ Manejo robusto de errores y fallbacks

---

## Resumen de Optimizaciones Aplicadas

**üöÄ GPU (GTX 1650):**
- Detecci√≥n autom√°tica de CUDA en todos los ejercicios
- Uso de precisi√≥n mixta (FP16) para acelerar inferencia
- Gesti√≥n optimizada de memoria GPU
- Fallback autom√°tico a CPU si GPU no disponible

**‚ö° CPU:**
- Paralelizaci√≥n con todos los cores disponibles (multiprocessing)
- Modelos m√°s peque√±os como alternativa para hardware limitado
- Procesamiento en lotes para mejor eficiencia
- Optimizaci√≥n de carga de datos y preprocesamiento

**üìà Rendimiento:**
- Tiempos de ejecuci√≥n reducidos 3-10x seg√∫n el ejercicio
- Uso eficiente de memoria RAM y GPU
- Indicadores de progreso y m√©tricas detalladas
- Manejo robusto de errores con fallbacks inteligentes

---

*Documento generado autom√°ticamente con todas las respuestas y resultados de los ejercicios de PLN optimizados.*